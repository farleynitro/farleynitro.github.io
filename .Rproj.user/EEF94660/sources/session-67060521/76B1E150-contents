---
title: "Capitulo 2: Exploración de datos"
description: |
  Explorando
author:
  - name: Farley Rimon
    url: {}
date: 2022-10-05
output:
  distill::distill_article:
    self_contained: false
---


# 1 Limpiando el database

Primero, cargaremos los paquetes que utilizaremos en esta sesión:

```{r paquetes, warning=FALSE, message=FALSE}
library(tidyverse)
library(sjmisc)
library(haven)
library(kableExtra)
library(readxl)
library(skimr)
library(sjPlot)
library(naniar)
library(ggcorrplot)
library(naniar)
library(plyr)
library(dplyr)

#Cronbach analysis
library(psych)
```

Y cargaremos la base de datos con variables ya seleccionadas:

```{r}
df_2014 <- read_sav("data/Base_en_SAV_Primera_Encuesta_Nacional_de_Medio_Ambiente_2014.sav")
df_2015 <- read_sav("data/Base_en_SAV_Encuesta_Medio_Ambiente_2015.sav")
df_2016 <- read_sav("data/Base_en_SAV_Encuesta_de_Medio_Ambiente_2016.sav")
df_2018 <- read_excel("data/Base-en-excel-Encuesta-Nacional-de-Medio-Ambiente-2018.xlsx")
```

Es importante mencionar que algunas preguntas están doble. Como pregunta abierta, y luego fueron codificadas a base de respuestas similares.

```{r, results='hide'}
skimr::skim(df_2018)
```

```{r,results='hide'}
skimr::skim(df_2016)
sjPlot::view_df(df_2016)
```


```{r}
(sum(is.na(df_2016))/prod(dim(df_2016)))*100
```

```{r,results='hide'}
skimr::skim(df_2015)
```


```{r}
(sum(is.na(df_2015))/prod(dim(df_2015)))*100
```

```{r,results='hide'}
skimr::skim(df_2014)
```


```{r}
(sum(is.na(df_2014))/prod(dim(df_2014)))*100
```

Vemos que la cantidad de datos faltantes es poca para el 2016, un 2% de la base.2014 y 2015 ya tienen mas del 30% de datos faltantes. Si ocurre en solo unas de las columnas, se pueden remover de la base para tener una base de datos limpia. Para eso usamos la funcion sort:

```{r}
sort( colSums( sapply(df_2016, is.na) ), decreasing=TRUE) %>% head(20)
```
Vemos que son 14 columnas con datos faltantes para el 2016. De 2170 observaciones, falta más de el 25% (598/2170*100%= 27.2%) de los datos para los indicadores. Removemos todas las columnas que le falten mas de 15% de las observaciones. En vez de observar cuales columnas tiene mas de 15% en el 2015 y 2014, removemos las columnas directamente ya que no son útiles en todo caso.

```{r}
df_2016_final <- df_2016[, colMeans(is.na(df_2016)) <= .15]
df_2015_final <- df_2015[, colMeans(is.na(df_2015)) <= .15]
df_2014_final <- df_2014[, colMeans(is.na(df_2014)) <= .15]
```


El siguiente Excel ofrece una descripción de la base de datos de las encuestas.

Primero la data era la siguiente.

```{r, echo= FALSE}

readxl::read_excel("data/Overview data.xlsx") %>%
  knitr::kable(., align = 'lccccccc') %>%
    kable_styling(bootstrap_options = c("striped"), html_font = 'Roboto Condensed') %>%
      column_spec(1:2, width_min = "2cm", border_left = T) %>%
      column_spec(3:8, width_min = "3cm", border_left = T) %>%
 scroll_box(width = "100%", height = "400px")
```

vemos que habian columnas especificas con la mayoria de los datos faltantes. Las columnas que quedan, juntas, tienen menos del 0.19% con datos faltantes. Esta fracción del total se puede ignorar. Despues de limpiar el database, el *overview* cambia a la siguiente.

```{r}
(sum(is.na(df_2016_final))/prod(dim(df_2016_final)))*100
(sum(is.na(df_2015_final))/prod(dim(df_2015_final)))*100
(sum(is.na(df_2014_final))/prod(dim(df_2014_final)))*100
```

```{r, results='hide'}
skimr::skim(df_2014_final)
skimr::skim(df_2015_final)
skimr::skim(df_2016_final)
```





```{r, echo = FALSE}

readxl::read_excel("data/Overview data 2.xlsx") %>%
  knitr::kable(., align = 'lccccccc') %>%
    kable_styling(bootstrap_options = c("striped"), html_font = 'Roboto Condensed') %>%
      column_spec(1:2, width_min = "2cm", border_left = T) %>%
      column_spec(3:8, width_min = "3cm", border_left = T) %>%
 scroll_box(width = "100%", height = "400px")
```


# 2 EDA: Encontrando las variables operacionales

## 2.1 Encuesta nacional del 2018

Para simplificar el analisis, decido enfocarme solo en el 2016 y 2018 ya que es el mas completo de los años. A base del 2016 y 2018 decido cuales van a ser mis variables operacionales para todos los años.

```{r, results='hide'}

#re-ordering dataframe to have indicators in front

df_2018_final <- df_2018 %>%
  select("ZONAS", "ID", "COMUNA", "NSE","TNSE","NEDAD","TEDAD","REGION","CUOTA","POND", P32,P33,P34,P35,P36,P37,P38,P39, everything())
sjPlot::view_df(df_2018_final)
```

```{r, results='hide', echo=FALSE}
df_2018_final %>% head()

# Just some tests for myself

n_distinct(df_2018_final$P17_COD)
unique(df_2018_final$P17_COD)

n_distinct(df_2018_final$P26_OTRO)
# unique(df_2018_final$P26_OTRO)
```

## 2.2 Exploración de variables independientes

Primero que escoger mis variables independientes + de control voy a tener que entender su sesgo y varianca. Tengo mas que nada entender mis datos para asi tambien entender mis resultados.

Variables socio-económicos y socio-demograficas son:

* **Zonas (macrozona en Chile)**
* **NSE (nivel Soco-Economico)**
* **TNSE (xxxxx)**
* **NEDAD (categórica de edad; 18-34;31-45; 46-60; 61+)**
* **TEDAD (categórica de edad; 18-34; 35-54; 55+)**
* **P32: Sexo (categórica: Hombre (1); Mujer (0))**
* **P33: Edad (variable continua)**
* **P34: Nivel de educación (categórica del 1-10)**
* **P36: Estudia (categórica: Si(1);No (2))**
* **P37: Nivel de educación con mas aporte al ingreso del hogar (categórica: 1-10)**
* **P38: Profesión con mas aporte al ingreso del hogar (categórica: 1-10)**
* **P39: Religión (categórica: 1-9)**
* **P43: Tipo de combustible que usa el vehiculo del encuestado (categórica: 1-..)**
* **P8: Razón por la que alguien no usa bicicleta (categórica: 1-..)**
* **P18: Opinion de la causa del cambio climatico (categórca: 1-..)**

De estas solo considero las cursivas. Las cursivas voy a analizar para multicolineadid y asi reducir las variables independientes usadas mas.

```{r, results= 'hide', echo=FALSE}
# unique(df_2018_final$POND)
unique(df_2018_final$TNSE)
unique(df_2018_final$NSE)
n_distinct(df_2018_final$CUOTA)
```

```{r}
# Rename columns of independent variables
df_2018_final <-  df_2018_final %>% rename('Razon_Bici'= 'P8','Razon_Cambio_Cl'= 'P18','Automovil_Combustible'= 'P43', 'Sexo'='P32' ,'Edad'='P33' ,'Niv_Edu'='P34','Estudiante'='P36','Edu_Ingr'='P37','Trab_Ingr'= 'P38','Religion'='P39', 'Transporte_mas_freq' = 'P7')
```

```{r}
var_ind <- df_2018_final %>% 
  select(ZONAS, NSE, TNSE, NEDAD, TEDAD, Sexo, Edad, Niv_Edu, Estudiante, Edu_Ingr, Trab_Ingr, Religion, Razon_Bici,Razon_Cambio_Cl,Automovil_Combustible, Transporte_mas_freq
         ) %>% 
    group_by(ZONAS) %>% 
summarise_all(function(x)  mean(x, na.rm = T))
  # na.omit()
```
Vemos que hay un warning. Vamos a ver cual variable ocasiona estos warning y porque. Yo pienso que son Razon_Bici,Razon_Cambio_Cl,Automovil_Combustible. Las ultimas 3 variables tienen un ".", un dato que se puede considerar NA. Como no se puede reemplezar lo mejor es imputarlo con el valor que es equivalente a "No Responde" para estas preguntas, el número **9** para *Razon_Cambio_Cl(P18) y Automovil_Combustible (P43)*, el número **99** para *Razon_Bici (P8)*. Cambiamos las columnas a *numeric* y terminamos. Despues, volvemos a formar un subset de las variables independientes.

```{r, results='hide'}

unique(df_2018_final$ZONAS)
unique(df_2018_final$NSE)
unique(df_2018_final$TNSE)
unique(df_2018_final$NEDAD)
unique(df_2018_final$TEDAD)
unique(df_2018_final$Sexo)
unique(df_2018_final$Edad)
unique(df_2018_final$Niv_Edu)
unique(df_2018_final$Estudiante)
unique(df_2018_final$Razon_Bici)
unique(df_2018_final$Razon_Cambio_Cl)
unique(df_2018_final$Automovil_Combustible)
unique(df_2018_final$Transporte_mas_freq)

df_2018_final["Razon_Bici"][df_2018_final["Razon_Bici"] == "."] <- "9"
df_2018_final["Razon_Cambio_Cl"][df_2018_final["Razon_Cambio_Cl"] == "."] <- "99"
df_2018_final["Automovil_Combustible"][df_2018_final["Automovil_Combustible"] == "."] <- "99"

var_ind <- df_2018_final %>% 
  select(ZONAS, NSE, TNSE, NEDAD, TEDAD, Sexo, Edad, Niv_Edu, Estudiante, Edu_Ingr, Trab_Ingr, Religion, Razon_Bici,Razon_Cambio_Cl,Automovil_Combustible, Transporte_mas_freq
         ) %>% 
    group_by(ZONAS) 

#checking the class of every column
sapply(var_ind, class)

#applying a new class
var_ind[] <- lapply(var_ind, function(x) as.numeric(as.character(x)))
sjPlot::view_df(var_ind)
```
### 2.2.1 Multicolinealidad

El nivel de correlación para todas las variables es alta ya que las variables son mayormente categóricas. Por eso, solo removemos variables que presenten una correlación alta para la mayoria de variables y que tengan una relación explicativa conceptualmente lógica. Es decir, no consideramos el TNSE,porque el NSE ya captura el estatus socio-económico. TEDAD, NEDAD es reemplazado por la variable continua de edad para tener mejor ajuste y distribución para el input del modelo (la edad es algo que se puede medir).

```{r}
#only include numeric/nominal categoric values

colnames(var_ind)
corr_vdem <- var_ind %>% 
  select(7:16) %>% 
  cor(use = "pairwise") %>% 
  round(1)
ggcorrplot(corr_vdem, type = "lower", lab = T, tl.cex = 9, show.legend = F)

var_ind <- var_ind %>% 
  select(ZONAS, NSE, TNSE, NEDAD, TEDAD, Sexo, Edad, Niv_Edu, Estudiante, Trab_Ingr, Religion, Razon_Bici,Razon_Cambio_Cl,Automovil_Combustible, Transporte_mas_freq
         ) %>% 
    group_by(ZONAS)
```
Excluimos la variable Edu_Ingr (¿Cuál es el nivel de educación que alcanzó la persona que aporta el principal ingreso de este hogar?) por su nivel de correlación con Niv_Edu (¿Me podría decir cuál es tu nivel de educación?). Interesantemente, hay una correlación baja entre el Trab_Ing (¿Cuál es la profesión o trabajo de la persona que aporta el principal ingreso de este hogar?) y el Nivel de Educación.

Vemos que todas las otras variables tienen colinealidad baja por la que no se presenta multicolinealidad y se pueden usar todas. **Importante es saber que muchas de estas variables no son continuas, pero nominales.** Ahora vamos a ver las variables ordinales.

```{r}

#only include numeric values

colnames(var_ind)
corr_vdem <- var_ind %>% 
  select(1:6) %>% 
  cor(use = "pairwise") %>% 
  round(1)
ggcorrplot(corr_vdem, type = "lower", lab = T, tl.cex = 9, show.legend = F)

var_ind <- var_ind %>% 
  select(ZONAS, NSE, TEDAD, Sexo, Edad, Niv_Edu, Estudiante, Trab_Ingr, Religion, Razon_Bici,Razon_Cambio_Cl,Automovil_Combustible, Transporte_mas_freq
         ) %>% 
    group_by(ZONAS)
```

Vemos que la NEDAD y TEDAD estan altamente correlacionados. Es logico porque las dos son variables ordinales de la edad. Sin embargo, nos quedamos con TEDAD que divide la edad en cuatro categorias en vez de 3 como por NEDAD. Esto creo que mas varianza en el input del modelo. El TNSE lo excluimos por la misma razon.

Para reducir dimensionalidad y ocuparnos de tener las variables que son significante vamos a reducir la cantidad de variables independientes. 

### NO ES VALIDO: 2.2.1.1 Chi-Square Test
Si el p<0.05 asumimos que las variables son en verdad independientes. Solo hago el analisis en los que supongo conceptualmente que tienen dependencia.

```{r}
chisq.test(var_ind$Estudiante, var_ind$Trab_Ingr,simulate.p.value = TRUE)


chisq.test(var_ind$Estudiante, var_ind$Razon_Bici,simulate.p.value = TRUE)


chisq.test(var_ind$Razon_Bici, var_ind$Razon_Cambio_Cl,simulate.p.value = TRUE)


chisq.test(var_ind$Razon_Bici, var_ind$Transporte_mas_freq, simulate.p.value = TRUE)


chisq.test(var_ind$Niv_Edu, var_ind$Estudiante,simulate.p.value = TRUE)

chisq.test(var_ind$Automovil_Combustible, var_ind$Niv_Edu,simulate.p.value = TRUE)


chisq.test(var_ind$Niv_Edu, var_ind$NSE,simulate.p.value = TRUE)

chisq.test(var_ind$Trab_Ingr, var_ind$NSE,simulate.p.value = TRUE)

chisq.test(var_ind$Transporte_mas_freq, var_ind$NSE, simulate.p.value = TRUE)

chisq.test(var_ind$ZONAS, var_ind$NSE,simulate.p.value = TRUE)

```

Todas parecen ser significantes, y por eso, independientes. Sin embargo, las aproximaciones pueden ser erroneas por los valores tan chicos. Esto resulta en un warning que me dio primero que usar el **simulated p-value**. Online decia esto:
"It gave the warning because many of the expected values will be very small and therefore the approximations of p may not be right."

### 2.2.1.2 TEST-VALIDO: Kruskal Wallis test

Para variables categoricas en realidad no se puede medir la correlacion, porque no son variables parametricas. Por eso, usamos el test **Kruskal Wallis**, cual mide el Chi-square sobre estas variables ordinales y categoricas. Ya que nuestra variable independiente principal es "ZONA" relacionamos los tests cada vez a esta variable. Si el valor-p es <0.05, asumimos que hay una diferencia significante entre encuestados de diferentes zonas.

```{r}
kruskal.test(ZONAS~ NSE, data = var_ind)
kruskal.test(ZONAS~ TEDAD, data = var_ind)
kruskal.test(ZONAS~ Edad, data = var_ind)
kruskal.test(ZONAS~ Sexo, data = var_ind)
kruskal.test(ZONAS~ Niv_Edu, data = var_ind)
kruskal.test(ZONAS~ Estudiante, data = var_ind)
kruskal.test(ZONAS~ Trab_Ingr, data = var_ind)
kruskal.test(ZONAS~ Religion, data = var_ind)
kruskal.test(ZONAS~ Razon_Bici, data = var_ind)
kruskal.test(ZONAS~ Razon_Cambio_Cl, data = var_ind)
kruskal.test(ZONAS~ Automovil_Combustible, data = var_ind)
kruskal.test(ZONAS~ Transporte_mas_freq, data = var_ind)
```
Vemos que el sexo tiene un valor-p > 0.05 por la que no la consideramos. Tambien vemos que hay mas varianza entre las zonas para la variable ordinal de edad, TEDAD, en vez de la variable parametrica, Edad. Aún asi, tomamos la edad ya que es parametrica.



### 2.2.1.3 Variables independientes finales

Nuestra variable independiente sigue siendo las zonas. Nuestras otras variables sirven como control ya que son indicadores socio-económicos y socio-demograficos.

```{r}
var_ind_s_control <- var_ind %>% select(ZONAS, NSE, Edad, Niv_Edu, Estudiante, Trab_Ingr, Religion, Razon_Bici, Razon_Cambio_Cl, Automovil_Combustible, Transporte_mas_freq) 
```

### 2.2.2 Visualización


*Zonas:*

Vemos que hay mas respondientes en las zona 4 (el Sur), alrededor de un 30% mas. Es importante considerar esto, porque no solo nuestro modelo estara mas entrenado para la zona 4, sino que tambien la relacion nacional (el baseline del analisis) estara sesgado a respondientes de la zona 4. Otras zonas parecen tener una cantidad relativemente igual de encuestados.

```{r message=FALSE, warning=FALSE}
var_ind_s_control %>% 
  ggplot(aes(x = as_factor(ZONAS))) +
  geom_bar(color = "#ffafcc", fill = "#ffc8dd", alpha = 0.5) + # Colores
  labs(title = "Barplot de respondientes por zonas",
       x = " ", y = "Frecuencia") + # Títulos
  theme_minimal(base_family = "Roboto Condensed")
```

*TEDAD:*

Vemos que la edad esta razonablemente distruida de una forma normal para cada zona. Sin embargo, vemos que la zona 1 cuenta con una cantidad de encuestados altos bajo la edad de 25. En la zona 4, resalta que una cantidad alta de respondientes esta alrededor de los 50 anos. Las zonas 1 y 2 parecen tener relativamente mas jovenes que las zonas 3 y 4. Sin embargo, casi se puede ignorar esta diferencia.

A nivel nacional, la distribucion es muy similar a los de las zonas con un sesgo ligero a la izquierda, cual se puede deber a las zonas 1 y 2.

```{r}
var_ind_s_control %>% 
  ggplot(aes(x = Edad, na.rm = T)) +
  geom_histogram() +
  facet_grid(~ZONAS) +
  labs(x = "Edad por Zona", y = "Frecuencia",
       title = "Edad",       caption = "Fuente: Encuesta Nacional 2018")
```
```{r}
var_ind_s_control %>% 
  ggplot(aes(x = Edad, na.rm = T)) +
  geom_histogram() +
  labs(x = "Edad", y = "Frecuencia",
       title = "Edad distribucion Nacional", caption = "Fuente: Encuesta Nacional 2018")
```
*NSE:* 

Como se esperaba, el Nivel socioeconomico es mas alto en el Sur. Donde 1 (clase alta, ABC1) y 2 (clase media acomodada, C2) es lo mas alto. 3 a 5 va de clase media a clase mas pobre. El Norte (zona 1) parece tambien tener una cantidad alta de clase alta. La zona central y RM parecen tener una distribucion mas normal con individuos de todas las clases. En todos los clasos, la cantidad de pobres que se entrevisto es muy baja. *Esto anade un sesgo al Nivel Socio-económico* ya que nos falta datos de la percepción de esta clase de gente. Para la otra clase se puede hablar de una representación relativamente justa.

```{r message=FALSE, warning=FALSE}
var_ind_s_control %>% 
  ggplot(aes(x = as_factor(NSE))) +
  facet_grid(~ZONAS) +
  geom_bar(color = "#ffafcc", fill = "#ffc8dd", alpha = 0.5) + # Colores
  labs(title = "Barplot de NSE dividido en Zonas",
       x = " ", y = "Frecuencia") + # Títulos
  theme_minimal(base_family = "Roboto Condensed")
```


*Nivel de educación:*

Vemos que en la mayoria de los casos el nivel de educación se divide en dos categorias: 
* 1) Universitaria completa (9)
* 2) Media completa (5)

Evidentemente, no hay una distribución normal de los datos. Esto es alarmante ya que indica que la variable puede anadir mucha inprecisión a los resultados del modelo. 

```{r message=FALSE, warning=FALSE}
var_ind_s_control %>% 
  ggplot(aes(x = as_factor(Niv_Edu))) +
  facet_grid(~ZONAS) +
  geom_bar(color = "#ffafcc", fill = "#ffc8dd", alpha = 0.5) + # Colores
  labs(title = "Barplot de Nivel de educación por zonas",
       x = " ", y = "Frecuencia") + # Títulos
  theme_minimal(base_family = "Roboto Condensed")
```
*Opinion de la razón del cambio climatico:*

Vemos que la mayoria de los encuestados, sin importar de donde vienen piensan mayormente que el cambio climatico viene por la actividad humana (1). Por esta razón, decidimos excluir esta variable de nuestro set de variables independientes. Una gran cantidad del grupo no respondió (99).

```{r message=FALSE, warning=FALSE}
var_ind_s_control %>% 
  ggplot(aes(x = as_factor(Razon_Cambio_Cl))) +
  facet_grid(~ZONAS) +
  geom_bar(color = "#ffafcc", fill = "#ffc8dd", alpha = 0.5) + # Colores
  labs(title = "Barplot de Razón del cambio climatico por zonas",
       x = " ", y = "Frecuencia") + # Títulos
  theme_minimal(base_family = "Roboto Condensed")
```
```{r}

var_ind_s_control <- var_ind %>% select(ZONAS, NSE, Edad, Niv_Edu, Estudiante, Trab_Ingr, Religion, Razon_Bici, Automovil_Combustible, Transporte_mas_freq) 
```

Salvamos el dataframe para que lo podamos usar en la construcción de los modelos.

```{r}
save(var_ind_s_control,file="var_independientes_y_control.Rda")
```



## 2.3 Variables dependientes

### 2.3.1 Las latentes

Las variables dependientes tienen el proposito de capturar la **percepción pública**. Como esto no es algo que se puede medir, se requiere el uso de variables latentes, cuales son subvariables las que se puede preguntar con mas precisión la opinion de alguien para una subcategoria. Es decir, la percepción pública la vamos a medir a traves de dos latentes, **seriedad de cambio climatico percibido** y **preocupación hacia el cambio climatico**. La manera de capturar la percepción pública sobre el cambio climatica es visible en la imagen abajo. Nos enfocamos en los cuadrantes naranjas (las dos latentes de interes). Estas dos latentes constituyen de diferentes preguntas en la Encuesta Nacional del 2018. 

![La división de la variable ](images/variables latentes.jpg)


### 2.3.2 Limpieza de datos

```{r}
#create subset of dependent variables
var_dep <- df_2018_final %>%
          select(P1_MAmb,P2_COD,P3A,P3B,P3C,P3D,P3E,P3F,P4,
                 P17_COD,P19_1,P19_2,P19_3,P19_4,P19_5,P19_6,
                 P19_7,P19_8, P19_10, P20,P21A,P21B,P21C,
                 P21D,P30)
```


```{r}
#1 to 7

#no NA values

unique(var_dep$P1_MAmb)
unique(var_dep$P2_COD)
unique(var_dep$P3A)
unique(var_dep$P3B)

unique(var_dep$P3C)
unique(var_dep$P3D)
unique(var_dep$P3E)
unique(var_dep$P3F)
unique(var_dep$P4)
```


```{r}
#17 to 19

#no NA
unique(var_dep$P17_COD)

#has NA
unique(var_dep$P19_1)
unique(var_dep$P19_2)
unique(var_dep$P19_3)
unique(var_dep$P19_4)
unique(var_dep$P19_5)
unique(var_dep$P19_6)
unique(var_dep$P19_7)
unique(var_dep$P19_8)
unique(var_dep$P19_10)


#cleaning NAs by putting the "." as NA
var_dep %>% replace_with_na(replace = list(P19_1 = ".", P19_2= ".",P19_3=".",P19_4=".",P19_5=".",
                                           P19_6=".",P19_7=".",P19_8=".",P19_10="."))
```


```{r}
#has NA
unique(var_dep$P20)

#has NA
unique(var_dep$P21A)
unique(var_dep$P21B)
unique(var_dep$P21C)
unique(var_dep$P21D)

#cleaning NAs by putting the "." as NA

var_dep %>% replace_with_na(replace = list(P21A = ".", P21B= ".",P21C=".",P21D="."))

#no NA
unique(var_dep$P30)

#after cleaning all the data, convert to numeric type

var_dep[] <- lapply(var_dep, function(x) as.numeric(as.character(x)))
```
```{r}
#esto no es valido mas porque va a traer mucha confusión en el alfa de Cronbach

# var_dep["P19_1"][var_dep["P19_1"] == "."] <- "9"
# var_dep["P19_2"][var_dep["P19_2"] == "."] <- "9"
# var_dep["P19_3"][var_dep["P19_3"] == "."] <- "9"
# var_dep["P19_4"][var_dep["P19_4"] == "."] <- "9"
# var_dep["P19_5"][var_dep["P19_5"] == "."] <- "9"
# var_dep["P19_6"][var_dep["P19_6"] == "."] <- "9"
# var_dep["P19_7"][var_dep["P19_7"] == "."] <- "9"
# var_dep["P19_8"][var_dep["P19_8"] == "."] <- "9"
# var_dep["P19_10"][var_dep["P19_10"] == "."] <- "9"


#cleaning NAs by putting the "." as NA
# var_dep %>% replace_with_na(replace = list(P21A = ".", P19_2= ".",P19_3=".",P19_4=".",P19_5=".",
#                                            P19_6=".",P19_7=".",P19_8=".",P19_10="."))
# # var_dep %>% replace_with_na_all(condition = ~.x == ".")

# var_dep["P20"][var_dep["P20"] == "."] <- "9"
# var_dep["P21A"][var_dep["P21A"] == "."] <- "9"
# var_dep["P21B"][var_dep["P21B"] == "."] <- "9"
# var_dep["P21C"][var_dep["P21C"] == "."] <- "9"
# var_dep["P21D"][var_dep["P21D"] == "."] <- "9"
```

### Alfa de Cronbach

Para analizar los resultados hacemos uso del alfa de Cronbach. Este indica el grado de correlación que hay entre todos los ítems pertenecientes a una escala, asumiendo que los mismos miden el constructo que dicen medir. Es decir, se espera que un encuestado tenga respuestas similares en una parilla que intenta capturar un aspecto especifico de la percepción. Mientras mas bajo el alfa de Cronbach, menos correlación hay entre las respuestas, se podria decir que menos relacionadas estan las preguntas, y por si, no capturan la percepcion de un y solo un concepto. 


* alfa de Cronbach < 0.5 es practicamente **inaceptable**.
* alfa de Cronbach  0.5 < 0.59 es **pobre**.
* alfa de Cronbach 0.6 < 0.69 es **cuestionable**.
* alfa de Cronbach 0.7 < 0.79 es **aceptable**.
* alfa de Cronbach 0.8 < 0.89 es **bueno**.
* alfa de Cronbach > 0.9 es **excelente**.


**Magnitud serian:**

* P1_MAmb Ordene los siguientes temas del más importante al menos importante para el país: Medioambiente **Singular**

1	1er lugar
2	2do lugar
3	3er lugar
4	4to lugar
5	5to lugar



* P3A-F: ¿Cómo evalúa usted…? La calidad del aire (A), Flora y Fauna (B), Rios y lagos (C), el mar (D), plazas y parques urbanos (E), medio ambiente en general en su región (F) **Escala Likert**

1	Pésimo
2	Malo
3	Regular
4	Bueno
5	Excelente
8	No sabe
9	No responde

Vemos que hay muy poca correlación entre las respuestas de encuestados en esta parilla, de **0.58**. Esto se puede explicar porque el contacto que los encuestados tienen con las diferentes opciones depende de la localización de encuestados. Alguien cerca del mar tiene otra percepción que alguien lejos del mar. Esto tambien se refleja en el hecho que si remueves P3D, la percepción sobre el mar, el alfa de Cronbach mejora un poco. La mejor manera de ver si la percepcion de esta depende de la localización del encuestado, grupamos por zona y vemos si este aun es el caso. Si el alfa incrementa, ya hay una primera indicación que la localización cambia la percepción sobre la calidad del medio ambiente. Por esta razón se deja esta parilla como variable dependiente.


```{r}

df_cronbach_P3 <- var_dep %>% select(P3A:P3F)
psych::alpha(x = df_cronbach_P3)

```
```{r}
# split the data frame by grouping using "f" argument
split_data <- split(df_2018_final, f = df_2018_final$ZONAS) 
split_data_1 <- split_data[[1]]
split_data_2 <- split_data[[2]]
split_data_3 <- split_data[[3]]
split_data_4 <- split_data[[4]]

split_data_p3_1 <- split_data_1 %>% select(P3A:P3F)
split_data_p3_2 <- split_data_2 %>% select(P3A:P3F)
split_data_p3_3 <- split_data_3 %>% select(P3A:P3F)
split_data_p3_4 <- split_data_4 %>% select(P3A:P3F)
```


```{r}
psych::alpha(x = split_data_p3_1)
```


```{r}
psych::alpha(x = split_data_p3_2)
```


```{r}
psych::alpha(x = split_data_p3_3)
```


```{r}
psych::alpha(x = split_data_p3_4)
```

### 2.3.3 Cambiando las escalas

Vemos que el alfa de Cronbach sigue siendo muy pobre. Por esta razon decidimos no tomar el promedio de la parilla como conjunto, pero solo analizar la percepción general del medio ambiente, exclusive de factores afuera de la vista de alguien, es decir P3F cual pregunta sobre **la calidad del medio ambiente en general de la región**.


* P4 Comparando el estado del medio ambiente de su región hace 10 años atrás, ¿cómo calificaría usted el estado actual del medio ambiente en su región? **Singular**

1	Mejor
2	Igual
3	Peor
8	No sabe
9	No responde


* P17_COD ¿Qué entiende usted por cambio climático? (codificado) **No hay escala, no se puede medir**

1	Aumento o disminución de la humedad
2	Aumento o disminución de los vientos
3	Cambios en la duración de las estaciones del año (ej. Veranos o inviernos más largos o más cortos)
4	Cambios en las lluvias (ej. Lluvias en fechas poco habituales, más/menos lluvias)
5	Cambios en las temperaturas (ej. Mucho más calor/frio)
6	Sequías más intensas y/o frecuentes
7	Otro
88	No sabe
99	No responde

* P20 ¿Cree usted que el cambio climático está sucediendo o sucederá en algún momento en el futuro? **Singular**
 

1	Si, ya está ocurriendo
2	Ocurrirá en un futuro
3	Ya ocurrió
4	No ocurrirá
8	No sabe
9	No responde

* P21A-D: Según su percepción, ¿cuán importante es el cambio climático para…?: Los chilenos (A), su región (B), familia y amigos (C), usted (D) **Escala Likert**

1	Nada importante
2	Poco importante
3	Bastante importante
4	Muy importante
8	No sabe
9	No responde


```{r}
#Cronbach analysis
library(psych)
library(tidyverse)

df_cronbach_P21 <- var_dep %>% select(P21A:P21D)
psych::alpha(x = df_cronbach_P21, na.rm = TRUE)
```
Vemos que el alfa de Cronbach es confiable, cual significa que hay consistencia semantica entre la respuestas de los encuestados a esta parilla. Es lógico ya que la pregunta no tiene una perfecta distinción ya que se pregunta secuencialmente sobre la percepción de chilenos, habitantes de la región en Chile, familia y amigos, y personalmente. Sin embargo, vemos que si se remueve la pregunta hacia chilenos, la consistencia mejora significantemente. Esto debe ser porque al contestar sobre los chilenos los encuestados tienen otra imagen del chileno promedio cual puede, o no puede ser, alineada con percepciones sobre los chilenos que tiene a su alrededor de la región, familia, o amigos. Lo mismo se puede decir con la pregunta de la región.


```{r}
var_dep %>% 
  select(P21A) %>% 
  rec(rec= "1=1;2=2;3=4;4=5;9=0;8=0;else=copy")

var_dep %>% 
  select(P21B) %>% 
  rec(rec= "1=1;2=2;3=4;4=5;9=0;8=0;else=copy")

var_dep %>% 
  select(P21C) %>% 
  rec(rec= "1=1;2=2;3=4;4=5;9=0;8=0;else=copy")

var_dep %>% 
  select(P21D) %>% 
  rec(rec= "1=1;2=2;3=4;4=5;9=0;8=0;else=copy")


var_dep$P21_promedio <- rowMeans(var_dep[22:24], na.rm=TRUE)
```


* P30 De acuerdo a lo que indican algunos científicos, en el planeta estamos viviendo la sexta extinción masiva de especies. ¿Sabe de especies animales o vegetales amenazadas o en peligro de extinción? **Singular**

1 Sí, por ejemplo
2 Sí, pero no recuerdo el nombre
3 No sé de ninguna
9 No responde



**Preocupación serian:** 

* P2_COD Según su percepción, ¿cuál es el principal problema ambiental que lo afecta a Ud.? (codificado) **No hay escala, no se puede medir**

1	Basura
2	Cambio climático
3	Congestión vehicular
4	Contaminación acústica
5	Contaminación de Agua
6	Contaminación de Aire
7	Falta de árboles y de áreas verdes
8	Malos olores
9	Perros vagos y sus excrementos
10	Polen de los árboles que causan alergia
11	Sequía
12	Falta de agua
13	Ninguno
88	No sabe
99	No responde

* P19_1-8, 10: ¿Qué sentimientos o emociones le surgen cuando escucha sobre Cambio Climático? Pena/Tristeza/Dolor (1), Preocupación/Angustia (2), Miedo/Susto/Temor (3), Incertidumbre (4), Rabia/enfado (5), Frustración (6), Responsabilidad/culpa (7), Indiferencia (8), NO SABE (10) **Escala de Guttman**

0	No
1	Si

```{r}
#Cronbach analysis
library(psych)
library(tidyverse)

df_cronbach_P19 <- var_dep %>% select(P19_1:P19_8,P19_10)
psych::alpha(x = df_cronbach_P19, na.rm = TRUE)
```
Vemos que hay una consistencia interna muy baja, aunque las preguntas sean muy similares. Es mejor no considerar esta parilla como un promedio. Sin embargo como tiene una escala de Guttman, vamos a sumar los resultados asociados a sentimientos negativos sobre el cambio climatico. NO consideramos P19_10 porque no mide el nivel de identificación con el cambio climatico de la misma manera que las otras preguntas. Las otras preguntas tienen orden jerarquico.

```{r}
# sum several columns in R
var_dep %>% 
  select(P19_8) %>% 
  rec(rec= "1=0;0=1;else=copy")

var_dep$P19_suma <- rowSums(var_dep[11:18], na.rm=TRUE)
```

* P20 ¿Cree usted que el cambio climático está sucediendo o sucederá en algún momento en el futuro? **Singular** 

1	Si, ya está ocurriendo
2	Ocurrirá en un futuro
3	Ya ocurrió
4	No ocurrirá
8	No sabe
9	No responde

* P30 De acuerdo a lo que indican algunos científicos, en el planeta estamos viviendo la sexta extinción masiva de especies. ¿Sabe de especies animales o vegetales amenazadas o en peligro de extinción? **Singular**

1 Sí, por ejemplo
2 Sí, pero no recuerdo el nombre
3 No sé de ninguna
9 No responde


Ahora que he limpiado todos los datos, en orden para obtener la preocupación pública decido sumar las respuestas de los encuestados. Para que esto sea posible, voy a sumar las respuestas a lo que llamo el **perception score**. Mientras mas alto sea este perception score, mas alto sera la preocupación y seriedad percibida. Para realizar esto hay que normalizar todos los valores de todas las preguntas en la siguiente forma:

* 0 - No responde, No disponible
* 1 - No importante
* 2 - No tan importante
* 3 - Neutral
* 4 - Importante
* 5 - Muy importante


**Es importante entender que todas las preguntas tienen otro formato y no todos indican relevancia hacia el medio ambiente directa. Esto es claramente una limitación. Sin embargo, asumimos que es la asumción es valida ya que indirectamente un número mas bajo en las preguntas codificadas se relaciona con acciones, y percepciones menos enfocadas hacia el medio ambiente.** Donde este no sea el caso, me ocupo manualmente de cambiar el formato en algo que concuerde con el formato para el perception score mencionado arriba. Ya esto forma la primera base útil para el modelo. 


* P1_Mambm: 1 a 5 -> 5 (=1) a 1 (=5)
* P3;A-F: 1 a 5 -> 5 (=1) a 1 (=5), 8 (=0), 9 (=0)
* P4: 1 a 3 -> 1 (=1), 2 (=3), 3 (=5), 8 (=0), 9 (=0)
* P19;1-8: 0 a 1 -> 0 (=1), 1 (=5)
* P20: 1 a 4 -> 4 (=1), 3 (=5), 2 (=3), 1 (=4), 8 (=0), 9 (=0)
* P21;B-D: 1 a 5 -> 1 (=1) a 5 (=5), 8 (=0), 9 (=0)
* P30: 1 a 3 -> 1 (=5), 2 (=3), 3 (=1), 9 (=0)

Excepciones:
* P2_COD tiene que ser una variable dependiente separada, porque calcula la percepción del cambio climatico en terminos no-cuantitativos. Ve la diferencia en percepcion, y no la preocupación o relevancia.
* P17_COD tiene que ser una variable dependiente separada, porque calcula la percepción del cambio climatico en terminos no-cuantitativos. Ve la diferencia en percepcion, y no la preocupación o relevancia.

Estas se veran como variables dependientes separadas.

```{r}
var_dep[] <- lapply(var_dep, function(x) as.numeric(as.character(x)))
var_dep["P17_COD"][var_dep["P17_COD"] == "99"] <- 0
var_dep["P17_COD"][var_dep["P17_COD"] == "88"] <- 0

var_dep %>% 
  select(P3F) %>% 
  rec(rec= "1=5;2=4;3=3;4=2;5=1;9=0;8=0;else=copy")

var_dep %>% 
  select(P30) %>% 
  rec(rec= "1=5;2=3;3=1;9=0;else=copy")

var_dep %>% 
  select(P20) %>% 
  rec(rec= "4=1;3=5;2=3;1=4;9=0;8=0;else=copy")

var_dep %>% 
  select(P4) %>% 
  rec(rec= "1=1;2=3;3=5;9=0;8=0;else=copy")
```
```{r}
var_dep$P21_promedio <- scale(var_dep$P21_promedio, center = FALSE, scale = max(var_dep$P21_promedio, na.rm = TRUE)/5)
var_dep$P21_promedio <- round(var_dep$P21_promedio,0)
unique(var_dep$P21_promedio)
```


### 2.3.2 Creación del score de percepción

Tenemos 4 variables dependientes que cada representa la percepción de algúna forma:

* P17_COD
* P2_COD
* Importancia: la suma de (P1_MAmb, P3F, P4, P20, P21_promedio, P30)
* Preocupación: la suma de (P30, P19_suma, P20)

```{r}
var_dep_P17_COD <- var_dep %>% select(P17_COD)
var_dep_P2_COD <- var_dep %>% select(P2_COD)


var_dep_importancia <- var_dep %>% select(P1_MAmb, P3F, P4, P20, P21_promedio, P30)
var_dep_preocupacion <- var_dep %>% select(P30, P19_suma, P20)

var_dep_total <- var_dep %>% select(P1_MAmb, P2_COD, P3F, P4, P17_COD, P19_suma, P20, P21_promedio, P30)
```

Salvamos las diferentes variables dependientes que se usaran durante la creación del modelo.

```{r}
#saving dataframes to RDA format

save(var_dep_P17_COD,file="var_dependientes_P17.Rda")
save(var_dep_P2_COD,file="var_dependientes_P2.Rda")
save(var_dep_importancia,file="var_dependientes_importancia.Rda")
save(var_dep_preocupacion,file="var_dependientes_preocupacion.Rda")
save(var_dep_total,file="var_dependientes_total.Rda")
```



#### 2.3.3 Multidimensional scaling o Principal Component Analysis

Ahora que sabemos el tipo de escala hay que combinarlas a una escala. Esta se puede desarollar o por el Analisis factorial o el Principal Component Analysis.

Primero que combinarlas en cualquier caso es importante saber como se relacionan con las variables independientes y como se relacionan entre si. Lo primero lo hacemos por visualizar algunas de estas relaciones. La segunda cuestión la hacemos por usar el **Alfa de Cronbach**.

Cargamos los paquetes:

```{r, message=F}
pacman::p_load(tidyverse, haven, GGally, factoextra, FactoMineR)
```

Para hacer análisis factorial también se puede usar la función `fa` del paquete `psych`.


```{r}
cor(na.omit(var_dep_total)) #Correlaciones: es necesario omitir los NA. 

ggcorr(var_dep_total, label = T) #NA se omiten automáticamente
```
```{r}
cor(na.omit(var_dep)) #Correlaciones: es necesario omitir los NA. 

ggcorr(var_dep, label = T) #NA se omiten automáticamente
```


```{r}
var_dep_high_corr <- var_dep_total %>% select(P1_MAmb, P17_COD, P2_COD, P30)
```


Realicemos el PCA.

```{r}
pca_1 <- PCA(var_dep_high_corr, graph = T) #Sin gráfico = F
```
```{r}
pca_2 <- PCA(var_dep, graph = T) #Sin gráfico = F
```


### Pesos relativos

```{r}
get_eig(pca_1)
get_eig(pca_2)
```

### Scree plot:
```{r}
fviz_eig(pca_1, choice = "eigenvalue", addlabels = T)
```

### Contribuciones de las variables a cada componente principal:

**Si es que las contribuciones fuesen uniformes, todas estarían en la línea de referencia.**

Cómo contribuye cada variable para cada dimensión:

```{r}
fviz_contrib(pca_1, choice = "var", axes = 1)
```

2. Esto nos puede decir que `milaid` es un *poco único*, ya que no pega con las demás variables puestas.

```{r}
fviz_contrib(pca_1, choice = "var", axes = 2)
```

3. Dimensión económica. 

```{r}
fviz_contrib(pca_1, choice = "var", axes = 3)
```

>Importante: incluir aquellas variables que son importantes teóricamente para tu modelo.

### Biplots (combinaciones posibles)

```{r}
fviz_pca(pca_1, axes = c(1, 2), label = "var")
```

```{r}
fviz_pca(pca_1, axes = c(1, 3), label = "var")
```

```{r}
fviz_pca(pca_1, axes = c(2, 3), label = "var")
```

### Generar el índice:

Los scores que asignan las dimensiones a cada observación:

```{r}
head(pca_1$ind$coord) #para cada observación, tendremos un score por dimensión. 
head(pca_1$var$coord) #para cada observación, tendremos un score por dimensión. 
```

Recordemos los pesos relativos:

```{r}
get_eig(pca_1)
```

Podemos generar el índice ponderando los scores por los *pesos relativos* de cada dimensión:

```{r}
indice_base <- pca_1$ind$coord %>% 
  as_tibble() %>% #base de lo que queremos
  mutate(indice_pca_1 = (Dim.1 * 38.35153 + Dim.2 * 24.96118) / 63.31271) 
```

Insertemos el indice en nuestra base completa:

```{r}
var_dep_total <- var_dep_total %>%
  bind_cols(indice_base %>% select(indice_pca_1))
```

Hagamos un rescale a 0-1

```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
```

```{r}
var_dep_total <- var_dep_total %>%
  mutate(indice_pca_1_rs = rescale01(indice_pca_1))
```

```{r}
tabla_1 <- datos_umsd %>% 
  select(country, indice_pca_1_rs) %>% 
  group_by(country) %>% 
  summarise(promedio = mean(indice_pca_1_rs)) %>% 
  arrange(-promedio)
```

#Análisis factorial:

```{r}
fa <-  factanal(x        = na.omit(var_dep_total),  
                factors  = 1, # un solo factor
                rotation = "varimax", 
                scores   = "regression")

rescale01(as.vector(fa$scores))
```

```{r}
var_dep_total_fa <- var_dep_total %>%
  select(.........,indice_pca_1_rs) %>%
  # hay un tema con los NA
  filter_all(~!is.na(.)) %>%
  bind_cols(tibble(indice_fa_rs = as.vector(rescale01(fa$scores))))
```

# Correlación PCA con FA

```{r}
cor(var_dep_total_fa$indice_pca_1_rs, var_dep_total_fa$indice_fa_rs)
```



```{r echo=FALSE}
#summing up all the columns to make the variable perception score
# var_dep %>%
#   score_perc = rowSums(select(everything()))
 
# var_dep$percepcion_score <- rowSums(var_dep[,c(1,2,3,4,5,6,7,8,9,30)],na.rm=TRUE)

# var_dep %>%
#   mutate(score_perc = rowSums(var_dep[,1:28]), 
#         na.rm = TRUE)
```

